import streamlit as st
import os
import time
from rag_system import RAGChatbot
import logging
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuraci√≥n de modelos y par√°metros desde .env
MODEL_CONFIG = {
    "llm_model": os.getenv("MODEL_NAME", "teknium/OpenHermes-2.5-Mistral-7B"),
    "embedding_model": os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-mpnet-base-v2"),
    "max_movies": int(os.getenv("MAX_MOVIES", "50000")),
    "max_response_length": int(os.getenv("MAX_RESPONSE_LENGTH", "1200")),
    "temperature": float(os.getenv("TEMPERATURE", "0.7")),
    "top_k": int(os.getenv("TOP_K", "8")),
    "quantization": os.getenv("QUANTIZATION", "4bit"),
    "load_in_4bit": os.getenv("LOAD_IN_4BIT", "true").lower() == "true"
}

# Nombres para mostrar en la UI
UI_NAMES = {
    "llm_display": "OpenHermes-2.5-Mistral-7B",
    "embedding_display": "MPNet-v2",
    "system_name": "Chatbot RAG IMDB - Sistema Avanzado de Consulta Cinematogr√°fica"
}

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Chatbot IMDB",
    page_icon="üé¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #ff6b6b, #4ecdc4);
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .chat-container {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .user-message {
        background-color: #007bff;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        text-align: right;
    }
    .bot-message {
        background-color: #28a745;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .source-box {
        background-color: #e9ecef;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
        font-size: 0.9em;
    }
    .stats-box {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #ffc107;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def initialize_chatbot():
    """Inicializa el chatbot optimizado para RTX 4070 (solo una vez)"""
    chatbot = RAGChatbot()
    # Usar configuraci√≥n optimizada desde .env
    success = chatbot.initialize(max_movies=MODEL_CONFIG["max_movies"], use_cache=True)
    return chatbot, success

def main():
    # Header principal
    st.markdown(f"""
    <div class="main-header">
        <h1 style="color: white; text-align: center; margin: 0;">
            üé¨ {UI_NAMES["system_name"]}
        </h1>
        <p style="color: white; text-align: center; margin: 0;">
            Potenciado por {UI_NAMES["llm_display"]} con RAG inteligente | {MODEL_CONFIG["max_movies"]:,}+ pel√≠culas de IMDB
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuraci√≥n del Sistema")
        
        # Informaci√≥n del hardware
        st.subheader("ÔøΩÔ∏è Hardware Detectado")
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                st.success(f"üöÄ GPU: {gpu_name}")
                st.info(f"üíæ VRAM: {gpu_memory:.1f} GB")
            else:
                st.warning("‚ö†Ô∏è GPU no detectada, usando CPU")
        except:
            st.warning("‚ö†Ô∏è No se pudo detectar hardware")
        
        st.divider()
        
        # Configuraci√≥n del modelo
        st.subheader("ü§ñ Configuraci√≥n del Modelo")
        temperature = st.slider("Temperatura", 0.1, 1.0, MODEL_CONFIG["temperature"], 0.1)
        max_length = st.slider("Longitud m√°xima de respuesta", 500, 1200, MODEL_CONFIG["max_response_length"], 50)
        top_k = st.slider("N√∫mero de documentos a recuperar", 3, 15, MODEL_CONFIG["top_k"])
        
        # Actualizar variables de entorno
        os.environ["TEMPERATURE"] = str(temperature)
        os.environ["MAX_RESPONSE_LENGTH"] = str(max_length)
        os.environ["TOP_K"] = str(top_k)
        
        st.divider()
        
        # Informaci√≥n del sistema
        st.subheader("üìä Estado del Sistema")
        if st.button("üîç Mostrar Informaci√≥n GPU"):
            try:
                import torch
                if torch.cuda.is_available():
                    memory_allocated = torch.cuda.memory_allocated(0) / 1e9
                    memory_cached = torch.cuda.memory_reserved(0) / 1e9
                    st.metric("VRAM Usada", f"{memory_allocated:.1f} GB")
                    st.metric("VRAM Reservada", f"{memory_cached:.1f} GB")
                else:
                    st.info("GPU no disponible")
            except:
                st.error("Error obteniendo informaci√≥n de GPU")
        
        # Bot√≥n para reinicializar
        if st.button("üîÑ Reinicializar Sistema"):
            st.cache_resource.clear()
            st.rerun()
    
    # Inicializar chatbot
    spinner_text = f"Inicializando sistema RAG con {UI_NAMES['llm_display']} y cuantizaci√≥n {MODEL_CONFIG['quantization']}... Esto puede tomar unos minutos la primera vez."
    with st.spinner(spinner_text):
        chatbot, success = initialize_chatbot()
    
    if not success:
        st.error("‚ùå Error inicializando el sistema. Por favor, verifica tu configuraci√≥n.")
        return
    
    # Mostrar estad√≠sticas del sistema
    stats = chatbot.get_stats()
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        status = "‚úÖ Optimizado" if stats["sistema_inicializado"] else "‚ùå Error"
        st.metric("Estado del Sistema", status)
    
    with col2:
        docs_count = f"{stats['documentos_en_base']:,}"
        st.metric("Documentos en Base", docs_count)
    
    with col3:
        # Extraer nombre corto del modelo para mostrar
        modelo_display = UI_NAMES["llm_display"]
        if stats["modelo_activo"]:
            modelo_completo = stats["modelo_activo"]
            if "OpenHermes" in modelo_completo:
                modelo_display = UI_NAMES["llm_display"]
            elif "zephyr" in modelo_completo.lower():
                modelo_display = "Zephyr-7B-Beta"
            elif "DialoGPT" in modelo_completo:
                modelo_display = "DialoGPT-Large"
            else:
                modelo_display = modelo_completo.split("/")[-1]
        st.metric("Modelo", modelo_display)
    
    with col4:
        kb_status = f"‚úÖ {UI_NAMES['embedding_display']} Optimizada" if stats["base_conocimiento_lista"] else "‚ùå Error"
        st.metric("Base de Conocimiento", kb_status)
    
    st.divider()
    
    # Interfaz principal de chat
    st.header("üí¨ Chat con el Asistente")
    
    # Inicializar historial de chat en session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
        welcome_message = f"""¬°Hola! Soy tu asistente cinematogr√°fico avanzado potenciado por {UI_NAMES["llm_display"]} con tecnolog√≠a RAG. Tengo acceso a m√°s de {MODEL_CONFIG["max_movies"]:,} pel√≠culas y series de IMDB con informaci√≥n detallada sobre actores, directores, g√©neros, calificaciones y mucho m√°s. 

üé¨ **¬øEn qu√© puedo ayudarte hoy?**
- Informaci√≥n sobre pel√≠culas espec√≠ficas
- Recomendaciones personalizadas
- Datos de actores y directores
- An√°lisis de g√©neros y tendencias
- Comparaciones entre pel√≠culas

¬°Preg√∫ntame lo que quieras sobre el mundo del cine!"""
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": welcome_message
        })
    
    # Mostrar historial de chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # Mostrar fuentes si las hay
            if message["role"] == "assistant" and "sources" in message:
                if message["sources"]:
                    with st.expander("üìö Fuentes consultadas"):
                        for i, source in enumerate(message["sources"], 1):
                            st.markdown(f"**Fuente {i}:**")
                            st.markdown(f"```\n{source}\n```")
    
    # Input del usuario
    if prompt := st.chat_input("Preg√∫ntame sobre pel√≠culas, actores, directores o cualquier tema cinematogr√°fico..."):
        # A√±adir mensaje del usuario al historial
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Mostrar mensaje del usuario
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generar respuesta del asistente
        with st.chat_message("assistant"):
            with st.spinner("Buscando informaci√≥n y generando respuesta..."):
                # Consultar al chatbot
                result = chatbot.query(prompt)
                
                # Mostrar respuesta
                st.markdown(result["response"])
                
                # Mostrar fuentes si las hay
                if result["sources"]:
                    with st.expander("üìö Fuentes consultadas"):
                        for i, source in enumerate(result["sources"], 1):
                            st.markdown(f"**Fuente {i}:**")
                            st.markdown(f"```\n{source}\n```")
                
                # A√±adir respuesta al historial
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": result["response"],
                    "sources": result["sources"]
                })
    
    # Secci√≥n de herramientas adicionales
    st.divider()
    st.header("üîß Herramientas Adicionales")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üîç B√∫squeda de Pel√≠culas")
        search_query = st.text_input("Buscar pel√≠cula por t√≠tulo:")
        
        if search_query:
            with st.spinner("Buscando pel√≠culas..."):
                movies = chatbot.search_movies(search_query, limit=5)
            
            if movies:
                for movie in movies:
                    with st.expander(f"üé¨ {movie.get('primaryTitle', 'T√≠tulo desconocido')} ({movie.get('startYear', 'N/A')})"):
                        st.write(f"**T√≠tulo original:** {movie.get('originalTitle', 'N/A')}")
                        st.write(f"**A√±o:** {movie.get('startYear', 'N/A')}")
                        st.write(f"**G√©neros:** {movie.get('genres', 'N/A')}")
                        st.write(f"**Duraci√≥n:** {movie.get('runtimeMinutes', 'N/A')} minutos")
                        st.write(f"**ID IMDB:** {movie.get('tconst', 'N/A')}")
            else:
                st.warning("No se encontraron pel√≠culas con ese t√≠tulo.")
    
    with col2:
        st.subheader("üìä Informaci√≥n del Sistema")
        with st.container():
            quantization_text = f"{MODEL_CONFIG['quantization']}" if MODEL_CONFIG["load_in_4bit"] else "FP16"
            stats_html = f"""
            <div class="stats-box">
                <h4>ü§ñ Sistema {UI_NAMES["llm_display"]}:</h4>
                <ul>
                    <li><strong>Modelo:</strong> {UI_NAMES["llm_display"]} (conversacional avanzado)</li>
                    <li><strong>Embeddings:</strong> {UI_NAMES["embedding_display"]} (comprensi√≥n sem√°ntica superior)</li>
                    <li><strong>Base de datos:</strong> {MODEL_CONFIG["max_movies"]:,}+ pel√≠culas y series de IMDB</li>
                    <li><strong>GPU:</strong> Cuantizaci√≥n {quantization_text} optimizada para RTX 4070</li>
                    <li><strong>Respuestas:</strong> Hasta {MODEL_CONFIG["max_response_length"]} tokens conversacionales</li>
                    <li><strong>Retrieval:</strong> {MODEL_CONFIG["top_k"]} documentos h√≠bridos (FAISS + BM25)</li>
                    <li><strong>Filtrado:</strong> Inteligente para temas cinematogr√°ficos</li>
                </ul>
                <h4>üí° Tipos de consultas soportadas:</h4>
                <ul>
                    <li><strong>Pel√≠culas:</strong> "¬øQu√© me puedes decir sobre Inception?"</li>
                    <li><strong>Actores:</strong> "Filmograf√≠a de Leonardo DiCaprio"</li>
                    <li><strong>Directores:</strong> "Mejores pel√≠culas de Christopher Nolan"</li>
                    <li><strong>G√©neros:</strong> "Pel√≠culas de ciencia ficci√≥n de los 90"</li>
                    <li><strong>Recomendaciones:</strong> "Recomi√©ndame una pel√≠cula familiar"</li>
                    <li><strong>Comparaciones:</strong> "Diferencias entre Batman (1989) y The Dark Knight"</li>
                </ul>
            </div>
            """
            st.markdown(stats_html, unsafe_allow_html=True)
    
    # Footer
    st.divider()
    footer_text = f"""
    <div style="text-align: center; color: #666; font-size: 0.9em;">
        üé¨ {UI_NAMES["system_name"]}<br>
        üöÄ {UI_NAMES["llm_display"]} + {UI_NAMES["embedding_display"]} | {MODEL_CONFIG["max_movies"]:,}+ pel√≠culas | Cuantizaci√≥n {MODEL_CONFIG["quantization"]} | Filtrado Inteligente<br>
        ‚ö° Powered by Hugging Face, FAISS, BM25 y Streamlit
    </div>
    """
    st.markdown(footer_text, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
